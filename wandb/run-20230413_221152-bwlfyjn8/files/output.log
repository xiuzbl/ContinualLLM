
































Loading checkpoint shards: 100%|██████████| 33/33 [02:04<00:00,  3.78s/it]
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization.
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'.
The class this function is called from is 'LlamaTokenizer'.
Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 8035.07it/s]
Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 11.75it/s]
100%|██████████| 1/1 [00:00<00:00, 16.76it/s]
Traceback (most recent call last):
  File "/mnt/data/yingxiu/LLMCODE/run.py", line 140, in <module>
    fire.Fire(main(args))
  File "/mnt/data/yingxiu/LLMCODE/run.py", line 70, in main
    train_val["train"].shuffle().map(lambda x: generate_and_tokenize_prompt(x, prompter, args)))
  File "/mnt/data/yingxiu/xiuenvs/alpaca3.9/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 563, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/mnt/data/yingxiu/xiuenvs/alpaca3.9/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 528, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/mnt/data/yingxiu/xiuenvs/alpaca3.9/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 3004, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/mnt/data/yingxiu/xiuenvs/alpaca3.9/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 3358, in _map_single
    example = apply_function_on_filtered_inputs(example, i, offset=offset)
  File "/mnt/data/yingxiu/xiuenvs/alpaca3.9/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 3261, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/data/yingxiu/LLMCODE/run.py", line 70, in <lambda>
    train_val["train"].shuffle().map(lambda x: generate_and_tokenize_prompt(x, prompter, args)))
  File "/mnt/data/yingxiu/LLMCODE/utils.py", line 31, in generate_and_tokenize_prompt
    tokenized_full_prompt = tokenize(full_prompt, args=args)
  File "/mnt/data/yingxiu/LLMCODE/utils.py", line 5, in tokenize
    result = tokenizer(
NameError: name 'tokenizer' is not defined
Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-0cb8404fb3cb3961/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...
Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0cb8404fb3cb3961/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e. Subsequent calls will reuse this data.